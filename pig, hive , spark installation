

Download pig from wget https://www.apache.org/dist/pig/pig-0.17.0/pig-0.17.0.tar.gz

untar this tar -xvzf pig-0.17.0.tar.gz
mv pig-0.17.0 /pig

vim /root/.bashrc or vim ~/.bashrc

export PIG_HOME=/pig
export PATH=$PATH:$PIG_HOME/bin
export PIG_CLASSPATH=$HADOOP_HOME/conf

exec bash or source ~/.bashrc


to run pig in local or map reduce system
pig -x local
pig -x mapreduce



###########################################################


Hive

wget http://www-eu.apache.org/dist/hive/hive-3.1.0/apache-hive-3.1.0-bin.tar.gz
ls
   72 tar -xvzf apache-hive-3.1.0-bin.tar.gz 
   73  ls
   74  mv apache-hive-3.1.0-bin /hive
   75  ls
   76  cd /
   77  ls
   78  cd
   79  vim /root/.bashrc
	export HIVE_HOME=/hive
	export PATH=$PATH:$HIVE_HOME/bin

   80  exec bash
   81  cd /hive/conf/
   82  ls
   83  vim hive-env.sh.template 
	export HADOOP_HOME=/hadoop2

   85  cd
	in Hadoop 1 we have to install derby tar file and give path to bashrc file 
	but in Hadoop 2 we have to type this command and derby meta file will install automatilly
   86  schematool -initSchema -dbType derby
	ls
	you will see derby.log

When u install hive there will be default directot will make in hdfs name tem/hive/root
You have to give permission to dirctory /tem/hive/root
hadoop fs -chmod 777 /tmp
hadoop fs -chmod 777 /tmp/hive
hadoop fs -chmod 777 /tmp/hive/root

to run hive on sysytem
[root@station199 ~]# hive


######################################################################
scala need for spark

wget https://scala-lang.org/files/archive/scala-2.12.6.tgz
tar -xvf scala-2.12.6.tgz
mv scala-2.12.6 /scala
vim /root/.bashrc
export PATH = $PATH:/scala/bin
exec bash
scala -version 
scala

###############################################

spark

wget https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
tar -xvf spark-2.3.0-bin-hadoop2.7.tgz
mv spark-2.3.0-bin-hadoop2.7 /spark

vim /root/.bashrc
export  SPARK_HOME=/spark
export PATH=$SPARK_HOME/bin:$PATH
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.6-src.zip:$PYTHONPATH
export PATH=$SPARK_HOME/python:$JAVA_HOME/bin:$PATH

exec bash

---to run spark
spark-shell

--- to run python on spark
pyspark

to see spark in browser
    namenode IP	       port
http://192.168.122.228:4042
###############################################

vim /root/.bashrc or vim ~/.bashrc

#Java
export JAVA_HOME=/usr/java/jdk1.8.0_121
export HADOOP_HOME=/hadoop2
export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

#Hive
export HIVE_HOME=/hive
export PATH=$PATH:$HIVE_HOME/bin
export  PATH

#Pig
export PIG_HOME=/pig
export PATH=$PATH:/pig/bin
export PIG_CLASSPATH=$HADOOP_HOME/conf

#Scala
export PATH=$PATH:/scala/bin

#Spark
export SPARK_HOME=/spark
export PATH=$SPARK_HOME/bin:$PATH
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.6-src.zip:$PYTHONPATH
export PATH=$SPARK_HOME/python:$JAVA_HOME/bin:$PATH

exec .bash or source ~/.bashrc




